#!/usr/bin/env python

import robotparser
import os
import sys

from argparse import ArgumentParser 
from urlparse import urlparse
from urllib2 import urlopen, Request, URLError
from time import time
from os.path import join as path_join

LIFE_TIME = 3600
USER_AGENT = 'robots/0.9'

def parse_args():
    parser = ArgumentParser()

    # Arguments
    parser.add_argument('url', help='url')

    # Options
    parser.add_argument('-a', '--as-user-agent', default='*',
                            help='test as user-agent')
    parser.add_argument('-c', '--cache-dir',
                            help='cache_dir directory')
    parser.add_argument('-l', '--life-time', default=LIFE_TIME,
                            help='display a square of a given number',
                            type=int)
    parser.add_argument('-u', '--user-agent', default=USER_AGENT,
                            help='user-agent header')

    return parser.parse_args()

def get_cache_entry(cache_dir, domain_name):
    entries = []
    cache_entry = path_join(cache_dir, domain_name)

    try:
        entries = os.listdir(cache_dir)
    except OSError:
        raise

    if domain_name in entries:
        try:
            cache_entry_stat = os.stat(cache_entry)
        except:
            raise

        return (cache_entry, cache_entry_stat.st_mtime)

    return (None, 0)

def update_cache_entry(cache_dir, url, *args, **kwargs):
    cache_entry = path_join(cache_dir, urlparse(url)[1])
    request = Request(url, **kwargs)

    remote_fp = None
    try:
        remote_fp = urlopen(request)
    except URLError:
        raise

    local_fp = None
    try:
        local_fp = open(cache_entry, 'wb')
    except IOError:
        raise

    if local_fp is not None and remote_fp is not None:
        local_fp.write(remote_fp.read())

    return cache_entry

def main():
    args = parse_args()

    # Hack to make robotparser send the correct user-agent.
    robotparser.URLopener.version = args.user_agent
    
    cache_entry = None
    cached = False

    url = urlparse(args.url)
    robots_url = '{0}://{1}/robots.txt'.format(url[0], url[1])

    cache_entry = None
    if args.cache_dir is not None:
        (cache_entry, mtime) = get_cache_entry(args.cache_dir, url[1])

    if args.cache_dir is not None:
        if time() - mtime >= args.life_time:
            cache_entry = update_cache_entry(args.cache_dir, robots_url,
                            headers={'User-Agent':args.user_agent})
        robots_url = cache_entry

    robot_parser = robotparser.RobotFileParser(robots_url)
    robot_parser.read()

    if robot_parser.can_fetch(args.as_user_agent, sys.argv[1]):
        sys.exit(0)

    sys.exit(1)


if __name__ == '__main__':
    main()
